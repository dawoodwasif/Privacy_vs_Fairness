{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from models.Nets import CNNMnist  # Import the correct model class\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils.options import args_parser  # Import the args_parser function\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily override sys.argv\n",
    "import sys\n",
    "sys.argv = ['']  # Clears any command-line arguments\n",
    "\n",
    "# Parse arguments\n",
    "args = args_parser()\n",
    "\n",
    "# Override specific arguments\n",
    "args.model = 'cnn'\n",
    "args.num_channels = 1\n",
    "args.epochs = 5\n",
    "args.num_users = 10\n",
    "args.frac = 1.0\n",
    "\n",
    "# Set device\n",
    "args.device = torch.device('cuda' if args.gpu != -1 and torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"D:\\Dawood_Work\\Master_VT\\Privacy_vs_Fairness\\Privacy_vs_Fairness\\Hypothesis_1\\Chain-PPFL\\weights\\mnist_cnn_5_sample_dp_weights\\model_weights_final.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNMnist(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "net = CNNMnist(args=args)  # Ensure args is defined and appropriate for your model\n",
    "net.load_state_dict(torch.load(weights_path))\n",
    "net.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=args.local_bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=args.bs, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the model's confidence scores for both training and test data\n",
    "def get_confidence_scores(model, loader):\n",
    "    model.eval()\n",
    "    confidence_scores = []\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "        output = model(data)\n",
    "        softmax_output = torch.softmax(output, dim=1)\n",
    "        confidence, _ = torch.max(softmax_output, dim=1)\n",
    "        confidence_scores.append(confidence.cpu().detach().numpy())\n",
    "    return np.concatenate(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence scores for training and testing data\n",
    "train_confidence = get_confidence_scores(net, train_loader)\n",
    "test_confidence = get_confidence_scores(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create labels (1 for train, 0 for test)\n",
    "train_labels = np.ones(len(train_confidence))\n",
    "test_labels = np.zeros(len(test_confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "all_confidence = np.concatenate([train_confidence, test_confidence])\n",
    "all_labels = np.concatenate([train_labels, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train an attack model (a simple logistic regression or MLP)\n",
    "# Split the data into a train and validation set for the attack model\n",
    "X_train, X_val, y_train, y_val = train_test_split(all_confidence.reshape(-1, 1), all_labels, test_size=0.5, random_state=args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a simple logistic regression attack model\n",
    "attack_model = LogisticRegression()\n",
    "attack_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate the attack model\n",
    "preds = attack_model.predict(X_val)\n",
    "attack_success_rate = accuracy_score(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membership Inference Attack Success Rate: 0.86\n"
     ]
    }
   ],
   "source": [
    "print(f\"Membership Inference Attack Success Rate: {attack_success_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed.\n",
      "Epoch 1 completed.\n",
      "Epoch 2 completed.\n",
      "Epoch 3 completed.\n",
      "Epoch 4 completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from models.Nets import CNNMnist  # Ensure you have the correct model\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "train_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = CNNMnist(args)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Differential Privacy parameters\n",
    "noise_multiplier = 5.0  # Controls the amount of noise added\n",
    "clip_value = 1.0  # Clipping threshold for gradients\n",
    "\n",
    "# Training loop with manual differential privacy\n",
    "model.train()\n",
    "for epoch in range(5):  # Adjust the number of epochs as needed\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Manually clip and add noise to gradients\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                # Gradient clipping\n",
    "                param.grad.data = torch.clamp(param.grad.data, -clip_value, clip_value)\n",
    "                \n",
    "                # Adding noise\n",
    "                noise = torch.normal(0, noise_multiplier * clip_value, size=param.grad.data.size()).to(device)\n",
    "                param.grad.data += noise\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} completed.\")\n",
    "\n",
    "# Save the differentially private model\n",
    "torch.save(model.state_dict(), 'dp_mnist_cnn_model_manual.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNMnist(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DP model\n",
    "# model = CNNMnist().to(device)\n",
    "# model.load_state_dict(torch.load('dp_mnist_cnn_model_manual.pth'))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Perform MIA (same approach as discussed previously)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membership Inference Attack Success Rate: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=args.local_bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=args.bs, shuffle=False)\n",
    "\n",
    "# Step 1: Get the model's confidence scores for both training and test data\n",
    "def get_confidence_scores(model, loader):\n",
    "    model.eval()\n",
    "    confidence_scores = []\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "        output = model(data)\n",
    "        softmax_output = torch.softmax(output, dim=1)\n",
    "        confidence, _ = torch.max(softmax_output, dim=1)\n",
    "        confidence_scores.append(confidence.cpu().detach().numpy())\n",
    "    return np.concatenate(confidence_scores)\n",
    "\n",
    "# Get confidence scores for training and testing data\n",
    "train_confidence = get_confidence_scores(net, train_loader)\n",
    "test_confidence = get_confidence_scores(net, test_loader)\n",
    "\n",
    "# Step 2: Create labels (1 for train, 0 for test)\n",
    "train_labels = np.ones(len(train_confidence))\n",
    "test_labels = np.zeros(len(test_confidence))\n",
    "\n",
    "# Combine data\n",
    "all_confidence = np.concatenate([train_confidence, test_confidence])\n",
    "all_labels = np.concatenate([train_labels, test_labels])\n",
    "\n",
    "# Step 3: Train an attack model (a simple logistic regression or MLP)\n",
    "# Split the data into a train and validation set for the attack model\n",
    "X_train, X_val, y_train, y_val = train_test_split(all_confidence.reshape(-1, 1), all_labels, test_size=0.2, random_state=args.seed)\n",
    "\n",
    "# Train a simple logistic regression attack model\n",
    "attack_model = LogisticRegression()\n",
    "attack_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the attack model\n",
    "preds = attack_model.predict(X_val)\n",
    "attack_success_rate = accuracy_score(y_val, preds)\n",
    "\n",
    "print(f\"Membership Inference Attack Success Rate: {attack_success_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values found in the input data. Handling NaN values...\n",
      "Membership Inference Attack Success Rate: 0.86\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from models.Nets import CNNMnist  # Import the correct model class\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils.options import args_parser  # Import the args_parser function\n",
    "import copy\n",
    "\n",
    "# Parse arguments\n",
    "args = args_parser()\n",
    "\n",
    "# Override specific arguments\n",
    "args.model = 'cnn'\n",
    "args.num_channels = 1\n",
    "args.epochs = 2\n",
    "args.num_users = 10\n",
    "args.frac = 1.0\n",
    "args.device = torch.device('cuda' if args.gpu != -1 and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the path to the saved DP model weights\n",
    "weights_path = 'D:\\Dawood_Work\\Master_VT\\Privacy_vs_Fairness\\Privacy_vs_Fairness\\Hypothesis_1\\Chain-PPFL\\weights\\mnist_cnn_5_sample_dp_weights\\model_weights_final.pth'\n",
    "\n",
    "# Initialize the model\n",
    "net = CNNMnist(args=args).to(args.device)\n",
    "net.load_state_dict(torch.load(weights_path))\n",
    "net.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=args.local_bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=args.bs, shuffle=False)\n",
    "\n",
    "# Step 1: Get the model's confidence scores for both training and test data\n",
    "def get_confidence_scores(model, loader, apply_dp=True):\n",
    "    model.eval()\n",
    "    confidence_scores = []\n",
    "    w_noise = copy.deepcopy(model.state_dict())\n",
    "    upsilon = 8  # Example DP noise factor, same as used in training\n",
    "    epsilon = 1e-10  # Small value to avoid division by zero\n",
    "\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "        \n",
    "        # Apply DP noise during evaluation\n",
    "        if apply_dp:\n",
    "            for lk in w_noise.keys():\n",
    "                if 'weight' in lk:  # Apply noise only to weight layers\n",
    "                    max_abs_val = torch.max(torch.abs(w_noise[lk]))\n",
    "                    if max_abs_val > 0:\n",
    "                        noised_sigma = float(2 * max_abs_val * args.local_ep) / float(args.epochs * upsilon + epsilon)\n",
    "                    else:\n",
    "                        noised_sigma = epsilon  # Prevent division by zero or very small scale\n",
    "\n",
    "                    # Ensure noised_sigma is a valid number\n",
    "                    if torch.isnan(torch.tensor(noised_sigma)) or noised_sigma <= 0:\n",
    "                        noised_sigma = epsilon\n",
    "\n",
    "                    distribution_laplace = torch.distributions.laplace.Laplace(0.0, noised_sigma)\n",
    "                    w_noise[lk] += distribution_laplace.sample(w_noise[lk].size())\n",
    "        \n",
    "        # Copy DP-affected weights into the model\n",
    "        model.load_state_dict(w_noise)\n",
    "        output = model(data)\n",
    "        softmax_output = torch.softmax(output, dim=1)\n",
    "        confidence, _ = torch.max(softmax_output, dim=1)\n",
    "        confidence_scores.append(confidence.cpu().detach().numpy())\n",
    "    return np.concatenate(confidence_scores)\n",
    "\n",
    "# Get confidence scores for training and testing data with DP applied\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Function to check and handle NaN values\n",
    "def handle_nan_values(X):\n",
    "    if np.isnan(X).any():\n",
    "        print(\"NaN values found in the input data. Handling NaN values...\")\n",
    "        # Option 1: Impute NaN values with the mean of the column\n",
    "        X = np.nan_to_num(X, nan=np.nanmean(X))\n",
    "        # Option 2: Remove rows with NaN values\n",
    "        # X = X[~np.isnan(X).any(axis=1)]\n",
    "    return X\n",
    "\n",
    "# Get confidence scores for training and testing data with DP applied\n",
    "train_confidence = get_confidence_scores(net, train_loader)\n",
    "test_confidence = get_confidence_scores(net, test_loader)\n",
    "\n",
    "# Combine data and handle NaN values\n",
    "all_confidence = np.concatenate([train_confidence, test_confidence]).reshape(-1, 1)\n",
    "all_confidence = handle_nan_values(all_confidence)\n",
    "\n",
    "# Labels\n",
    "train_labels = np.ones(len(train_confidence))\n",
    "test_labels = np.zeros(len(test_confidence))\n",
    "all_labels = np.concatenate([train_labels, test_labels])\n",
    "\n",
    "# Split the data into a train and validation set for the attack model\n",
    "X_train, X_val, y_train, y_val = train_test_split(all_confidence, all_labels, test_size=0.2, random_state=args.seed)\n",
    "\n",
    "# Train a simple logistic regression attack model\n",
    "attack_model = LogisticRegression()\n",
    "attack_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the attack model\n",
    "preds = attack_model.predict(X_val)\n",
    "attack_success_rate = accuracy_score(y_val, preds)\n",
    "\n",
    "print(f\"Membership Inference Attack Success Rate: {attack_success_rate:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
